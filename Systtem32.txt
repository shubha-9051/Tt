import gensim.downloader as api

model = api.load("glove-wiki-gigaword-50")

word1 = "paris"
word2 = "france"
word3 = "india"
word4 = "capital"

vector_add = model[word1] + model[word4]
vector_mix = model[word1] - model[word2] + model[word3]

result_add = model.most_similar([vector_add], topn=1)
result_mix = model.most_similar([vector_mix], topn=1)

print(f"The result of '{word2} + {word4}' is: {result_add[0][0]}")
print(f"The result of '{word1} - {word2} + {word3}' is: {result_mix[0][0]}")
-----------------------------------------------------------------------------------------
import gensim.downloader as api
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

print("Loading word vectors...")
model = api.load("glove-twitter-50")
print("Model loaded.")

words = ['wicket', 'innings', 'bowler', 'batsman', 'over', 'run', 'pitch',
         'boundary', 'catch', 'cricket']

valid_words = [word for word in words if word in model]
word_vectors = np.array([model[word] for word in valid_words])

tsne = TSNE(n_components=2, perplexity=5, random_state=0)
word_vectors_2d = tsne.fit_transform(word_vectors)

plt.figure(figsize=(8, 6))

for i in range(len(valid_words)):
    x = word_vectors_2d[i][0]
    y = word_vectors_2d[i][1]
    plt.scatter(x, y, color='blue')
    plt.text(x + 0.1, y + 0.1, valid_words[i])

plt.title("Cricket Word Embeddings (t-SNE Visualization)")
plt.grid(True)
plt.show()

if 'cricket' in model:
    print("\nWords most similar to 'cricket':")
    for similar_word, score in model.most_similar('cricket', topn=5):
        print(f"{similar_word:15} {score:.3f}")
else:
    print("'cricket' not found in the vocabulary.")
--------------------------------------------------------------------------------------------
import gensim
from gensim.models import Word2Vec
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

corpus = [
    "A patient with diabetes requires regular insulin injections.",
    "Medical professionals recommend exercise for heart health.",
    "Doctors use MRI scans to diagnose brain disorders.",
    "Antibiotics help fight bacterial infections but not viral infections.",
    "The surgeon performed a complex cardiac surgery successfully.",
    "Doctors and nurses work together to treat patients.",
    "A doctor specializes in diagnosing and treating diseases."
]

tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]

model = Word2Vec(
    sentences=tokenized_corpus,
    vector_size=100,
    window=3,
    min_count=1,
    workers=4,
    sg=1
)

model.save("medical_word2vec.model")

similar_words = model.wv.most_similar("doctor", topn=5)

print("\nWords most similar to 'doctor':")
for i, (word, similarity) in enumerate(similar_words, start=1):
    print(f"{i}. {word} (Similarity Score: {similarity:.2f})")
-------------------------------------------------------------------------------------------
from transformers import pipeline
import gensim.downloader as api

glove_model = api.load("glove-wiki-gigaword-50")

word = "technology"
similar_words = glove_model.most_similar(word, topn=5)
print(f"Similar words to '{word}': {similar_words}")

generator = pipeline("text-generation", model="gpt2")

def generate_response(prompt, max_length=100):
    response = generator(prompt, max_length=max_length, num_return_sequences=1)
    return response[0]['generated_text']

original_prompt = "Explain the impact of technology on society."
original_response = generate_response(original_prompt)

enriched_prompt = (
    "Explain the impact of technology, innovation, science, "
    "engineering, and digital advancements on society."
)
enriched_response = generate_response(enriched_prompt)

print("Original Prompt Response:")
print(original_response)
print("Enriched Prompt Response:")
print(enriched_response)
-------------------------------------------------------------------------------------------------
import gensim.downloader as gensim_api

glove_model = gensim_api.load("glove-wiki-gigaword-50")

def generate_paragraph(core_word, related_words):
    passage = (
        f"In a world shaped by {core_word}, innovation starts with a spark of {related_words[0][0]}. "
        f"Each step in {related_words[1][0]} reveals breakthroughs that redefine {related_words[2][0]}. "
        f"Engineers and thinkers collaborate to transform every {related_words[3][0]} into a leap toward "
        f"{related_words[4][0]}."
    )
    return passage

main_word = "technology"
top_matches = glove_model.most_similar(main_word, topn=5)

generated_output = generate_paragraph(main_word, top_matches)
print(generated_output)
----------------------------------------------------------------------------------------------------------------
from transformers import pipeline

print("Loading sentiment analysis model...")
sentiment_analyzer = pipeline("sentiment-analysis")
print("Model loaded successfully!")

test_texts = ["I love learning about AI"]

print("\nAnalyzing texts...\n")
for text in test_texts:
    result = sentiment_analyzer(text)[0]
    print(f"Text: {text}")
    print(f"Sentiment: {result['label']}")
    print(f"Confidence: {result['score']:.2f}")
-------------------------------------------------------------------------------------------------------------
from transformers import pipeline

summarizer = pipeline("summarization")

text = (
    "Artificial intelligence (AI) is a branch of computer science that aims to create intelligent machines "
    "that work and react like humans. Some of the activities computers with AI are designed for include speech "
    "recognition, learning, planning, and problem-solving. AI is being used across various industries including "
    "healthcare, finance, and transportation. It continues to evolve rapidly and is expected to have a significant "
    "impact on the future of work and everyday life."
)

summary = summarizer(text)

print(summary[0]['summary_text'])
-------------------------------------------------------------------------------------------------------------------
import os
from cohere import Client
from langchain.prompts import PromptTemplate

os.environ["COHERE_API_KEY"] = "Generate your own API key"
co = Client(os.getenv("COHERE_API_KEY"))

with open(r"C:\Users\Admin\Documents\AI.txt", "r", encoding="utf-8") as file:
    text_document = file.read()

template = """
You are an expert summarizer. Summarize the following text in a concise manner:
Text: {text}
Summary:
"""

prompt_template = PromptTemplate(input_variables=["text"], template=template)
formatted_prompt = prompt_template.format(text=text_document)

response = co.generate(
    model="command",
    prompt=formatted_prompt,
    max_tokens=40
)

print("Summary:")
print(response.generations[0].text.strip())
-----------------------------------------------------------------------------------------------------------------------
import wikipedia
from pydantic import BaseModel
from typing import Optional
import json

class InstitutionInfo(BaseModel):
    name: str
    founder: Optional[str]
    founding_year: Optional[int]
    summary: str

def get_institution_info(name: str) -> Optional[InstitutionInfo]:
    try:
        page_title = wikipedia.search(name)[0]
        page = wikipedia.page(page_title)
        content = page.content.lower()
        summary = wikipedia.summary(page_title, sentences=5)
        founder = next(
            (line.split("founded by")[1].split('.')[0].strip().title()
             for line in content.split('\n') if "founded by" in line),
            None
        )
        year = next(
            (int(word) for word in content.split() if word.isdigit() and len(word) == 4),
            None
        )
        return InstitutionInfo(
            name=page.title,
            founder=founder,
            founding_year=year,
            summary=summary
        )
    except Exception as e:
        print(f"Error: {e}")
        return None

if __name__ == "__main__":
    name = input("Enter Institution Name: ")
    info = get_institution_info(name)
    if info:
        print(json.dumps(info.dict(), indent=2))
------------------------------------------------------------------------------------------------------------------
from pypdf import PdfReader
from cohere import Client

co=Client("XgHOwe0YWKhnr5Tt8lxYl6QORGVGux7C2MXTOaop")
textdoc=PdfReader(r"file location")

text=""
for page in textdoc.pages:
    text+=page.extract_text()

def ipc(q):
    prompt=f"""
    you are a Indian penal code bot. use the following content to answer the user
    {text[:1000]}
    user question:{q}
    answer and be precise with ipc sections
    """
    res=co.generate(
        prompt=prompt,
        model="command-r-plus",
        max_tokens=300
    )
    print(""+res.generations[0].text.strip())
que=input("question:")
print(ipc(que))
-------------------------------------------------------------------------------------------------------------------------------------
from PyPDF2 import PdfReader
import re

text = "".join([page.extract_text() for page in PdfReader("ipc.pdf").pages[5:50]])

sections = [s.replace('\n', ' ').strip() for s in re.findall(r'(\d+\..*?)(?=\n\d+\.|\Z)', text, re.DOTALL)]

def ipc_bot(q):
    q = q.lower()
    for sec in sections:
        match = re.search(r'section\s*(\d+)', q)
        if match and sec.startswith(f"{match.group(1)}."):
            return sec.split('.', 1)[1].strip().split('.')[0] + "."
        if any(word in sec.lower() for word in q.split()):
            return sec.split('.', 1)[1].strip().split('.')[0] + "."
    return "No match."

while True:
    q = input("You: ")
    if q.lower() in ['exit', 'quit']:
        break
    print("IPC Bot:", ipc_bot(q))

